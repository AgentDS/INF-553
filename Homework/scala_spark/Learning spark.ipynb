{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.26.63.208:4041\n",
       "SparkContext available as 'sc' (version = 2.3.3, master = local[*], app id = local-1569957382357)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.HashPartitioner\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Programming with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:27\n",
       "inputRDD: org.apache.spark.rdd.RDD[String] = ./log.txt MapPartitionsRDD[2] at textFile at <console>:28\n",
       "errorsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at filter at <console>:29\n",
       "warningsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:30\n",
       "badLineRDD: org.apache.spark.rdd.RDD[String] = UnionRDD[5] at union at <console>:31\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.parallelize(List(\"pandas\", \"i like pandas\"))\n",
    "val inputRDD = sc.textFile(\"./log.txt\")\n",
    "val errorsRDD = inputRDD.filter(line => line.contains(\"error\"))\n",
    "val warningsRDD = inputRDD.filter(line => line.contains(\"warning\"))\n",
    "val badLineRDD = errorsRDD.union(warningsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input had 2 concerning lines\n",
      "Here are examples:\n",
      "The application closes the API session, resulting in an error being reported because the state of the flow is SESSIONED. This error can be ignored.\n",
      "Some interface types are not enabled for multicasting. Therefore, when the RSVP Agent tries to enable multicasting, a warning is reported. Such interfaces can still be used for unicasting.\n"
     ]
    }
   ],
   "source": [
    "println(\"Input had \" + badLineRDD.count() + \" concerning lines\")\n",
    "println(\"Here are examples:\")\n",
    "badLineRDD.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class SearchFunctions\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SearchFunctions(val query: String) {\n",
    "    import org.apache.spark.rdd.RDD\n",
    "    def isMatch(s: String): Boolean = {\n",
    "        s.contains(query)\n",
    "    }\n",
    "    \n",
    "    def getMatchesNoReference(rdd: RDD[String]): RDD[String] = { \n",
    "        // Safe: extract just the field we need into a local variable \n",
    "        val query_ = this.query \n",
    "        rdd.filter(x => x.contains(query_)) \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,4,9,16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:27\n",
       "result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at map at <console>:28\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = sc.parallelize(List(1,2,3,4))\n",
    "val result = input.map(x => x*x)\n",
    "println(result.collect().mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at <console>:29\n",
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at flatMap at <console>:30\n",
       "res2: String = hello\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.parallelize(List(\"hello world\", \"hi\"))\n",
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nums2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at <console>:27\n",
       "sum_: Int = 36\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nums2 = sc.parallelize(List(1,2,3,4,5,6,7,8))\n",
    "val sum_ = nums2.reduce((x,y) => x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: (Int, Int) = (10,4)\n",
       "avg: Double = 2.5\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = input.aggregate((0,0))(\n",
    "                              (acc,value)=>(acc._1 + value, acc._2 + 1),\n",
    "                              (acc1, acc2)=>(acc1._1 + acc2._1, acc1._2 + acc2._2))\n",
    "val avg = result._1 / result._2.toDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1,4,9,16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at map at <console>:31\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = input.map(x => x*x)\n",
    "println(result.count())\n",
    "println(result.collect().mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at map at <console>:29\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairs = lines.map(x => (x.split(\" \")(0), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[13] at filter at <console>:33\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.filter{case (key,value) => value.length < 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input: org.apache.spark.rdd.RDD[String] = ./log.txt MapPartitionsRDD[15] at textFile at <console>:34\n",
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[16] at flatMap at <console>:35\n",
       "result: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[18] at reduceByKey at <console>:36\n",
       "res5: Array[(String, Int)] = Array((index,7), (call,2), (dst-9.67.116.99:1047,3))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = sc.textFile(\"./log.txt\")\n",
    "val words = input.flatMap(x => x.split(\" \"))\n",
    "val result = words.map(x => (x,1)).reduceByKey((x,y) => x+y)\n",
    "result.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: scala.collection.Map[String,Long] = Map(:.......rsvp_action_nHop: -> 6, destination. -> 1, used -> 2, *************** -> 2, :......rpapi_getSpecData: -> 3, file: -> 1, down -> 1, enable -> 1, 08:53:52 -> 9, \"CLCat2\" -> 1, application -> 5, \"\" -> 808, reserved -> 1, PATH -> 10, for -> 60, papiUserValue -> 1, rapi_release() -> 1, :..........rpapi_Reg_UnregFlow: -> 12, 12 -> 2, =0 -> 1, TERM -> 1, 08 -> 2, PROTERR:.......rsvp_flow_stateMachine: -> 1, any -> 1, refresh -> 1, module -> 1, registered -> 1, name -> 4, this -> 2, gateway: -> 7, in -> 4, RESVED, -> 10, :........traffic_action_oif: -> 2, rsvp-udp -> 7, based -> 1, flow. -> 2, path. -> 1, reported -> 1, DELRESP -> 1, are -> 5, is -> 23, available. -> 4, source -> 7, [CL -> 1, informational -> 2, reported. -> 4, 19 -> 2, acce..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.flatMap(x => x.split(\" \")).countByValue() // much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:........rsvp_action_nHop:,2.0)\n",
      "(Tspec.,1.0)\n",
      "(APIInitialize:,3.0)\n",
      "(hop=9.67.116.99,,4.0)\n",
      "(down.,2.0)\n",
      "(flow[sess=9.67.116.99:1047:6,source=9.67.116.98:8000],1.0)\n",
      "(:..........qosmgr_response:,4.0)\n",
      "(src-9.67.116.98:8000,3.0)\n",
      "(:........flow_timer_start:,6.0)\n",
      "(changes,1.0)\n",
      "(rsvp-api,1.0)\n",
      "(RSVPPutActionName:,3.0)\n",
      "(call,,2.0)\n",
      "(TR1,1.0)\n",
      "([CL,1.0)\n",
      "(service=0,1.0)\n",
      "(TCP/IP,4.0)\n",
      "(08:54:35,23.0)\n",
      "(Associate,1.0)\n",
      "(action,2.0)\n",
      "(determines,1.0)\n",
      "(source=9.67.116.98:8000],3.0)\n",
      "(flow,7.0)\n",
      "(configured,1.0)\n",
      "(ReadBuffer:,12.0)\n",
      "(traffic,1.0)\n",
      "(SENDER,1.0)\n",
      "(expired,6.0)\n",
      "(terminate,2.0)\n",
      "(:........mailslot_send:,3.0)\n",
      "(interfaces,3.0)\n",
      "(:.......rsvp_action_nHop:,6.0)\n",
      "(STYLE,4.0)\n",
      "(is,23.0)\n",
      "(level,1.0)\n",
      "(30,1.0)\n",
      "(papi_debug:,3.0)\n",
      "(sessioned,1.0)\n",
      "(filter,2.0)\n",
      "(:......rpapi_getPolicyData:,11.0)\n",
      "(UNIX,1.0)\n",
      "(stopped,2.0)\n",
      "(rc=0,1.0)\n",
      "(querying,1.0)\n",
      "(if=9.67.116.98,1.0)\n",
      "(M,1.0)\n",
      "(Home,1.0)\n",
      "(does,2.0)\n",
      "(parameters,1.0)\n",
      "(#2,,1.0)\n",
      "(associates,1.0)\n",
      "(2,1.0)\n",
      "(interface.,1.0)\n",
      "(EDC8112I,1.0)\n",
      "(Such,1.0)\n",
      "(node,,1.0)\n",
      "(based,1.0)\n",
      "(through,1.0)\n",
      "((in,1.0)\n",
      "(can,2.0)\n",
      "(dump,2.0)\n",
      "(node,1.0)\n",
      "(tear,1.0)\n",
      "(This,3.0)\n",
      "(rapi_sender(),1.0)\n",
      "((9.67.116.99:1698),1.0)\n",
      "(client,1.0)\n",
      "(CLCat2,3.0)\n",
      "(5,1.0)\n",
      "(initialized.,1.0)\n",
      "(call,2.0)\n",
      "(been,4.0)\n",
      "(one,1.0)\n",
      "(items,1.0)\n",
      "(TCP,1.0)\n",
      "(:......mailslot_create:,3.0)\n",
      "(exist,1.0)\n",
      "(still,1.0)\n",
      "(used,2.0)\n",
      "(socket,5.0)\n",
      "(issues,2.0)\n",
      "(Get,1.0)\n",
      "(enters,3.0)\n",
      "(27,2.0)\n",
      "(mailbox,24.0)\n",
      "(rapi_session(),1.0)\n",
      "((due,1.0)\n",
      "(entity,7.0)\n",
      "(timer,2.0)\n",
      "(12,2.0)\n",
      "(not,9.0)\n",
      "(forwarded,1.0)\n",
      "(of,7.0)\n",
      "(session,,1.0)\n",
      "(rc,3.0)\n",
      "(being,3.0)\n",
      "(API,4.0)\n",
      "(TERM,1.0)\n",
      "(129.1.1.1,,2.0)\n",
      "(p,,1.0)\n",
      "(refreshed,1.0)\n",
      "(/u/user10/rsvpd1.conf,1.0)\n",
      "(09,2.0)\n",
      "(session,1.0)\n",
      "(installs,1.0)\n",
      "(:......rsvp_explode_packet:,4.0)\n",
      "(trace:,1.0)\n",
      "(type,3.0)\n",
      "(qoshandle=0,1.0)\n",
      "(log,1.0)\n",
      "(m=1024,1.0)\n",
      "(sent,2.0)\n",
      "(initializing,1.0)\n",
      "(:........traffic_action_oif:,2.0)\n",
      "(down,1.0)\n",
      "(made.,1.0)\n",
      "(:.....mailbox_register:,2.0)\n",
      "(issued,1.0)\n",
      "(15,3.0)\n",
      "(:..settcpimage:,2.0)\n",
      "(receiver,2.0)\n",
      "(reserved,1.0)\n",
      "(9.67.116.98,18.0)\n",
      "(values,2.0)\n",
      "(T1OUT,6.0)\n",
      "(DELRESP,1.0)\n",
      "(Reservation,1.0)\n",
      "(RegisterWithPolicyAPI:,3.0)\n",
      "(found,2.0)\n",
      "(ceased,1.0)\n",
      "(:..mailslot_sitter:,7.0)\n",
      "(outbound,1.0)\n",
      "(:...read_physical_netif:,8.0)\n",
      "(A,4.0)\n",
      "(:.........qosmgr_request:,4.0)\n",
      "(21,2.0)\n",
      "(rsvpd,1.0)\n",
      "(entry,7.0)\n",
      "(address,9.0)\n",
      "(or,1.0)\n",
      "(Therefore,,1.0)\n",
      "(service=1,,1.0)\n",
      "(0.0.0.0,7.0)\n",
      "(warning,1.0)\n",
      "(03,2.0)\n",
      "(#4,,1.0)\n",
      "(08:51:06,53.0)\n",
      "(get,1.0)\n",
      "(18,2.0)\n",
      "(application).,1.0)\n",
      "(sending,9.0)\n",
      "(rthdl-7f5251c8,1.0)\n",
      "(b,,1.0)\n",
      "(m,,1.0)\n",
      "(Result,6.0)\n",
      "(9.37.65.139,,2.0)\n",
      "(deregistering,1.0)\n",
      "(dst-9.67.116.99:1047,3.0)\n",
      "(every,2.0)\n",
      "(:......rsvp_event_propagate:,1.0)\n",
      "(:.....terminator:,1.0)\n",
      "(any,1.0)\n",
      "(24,2.0)\n",
      "(file,1.0)\n",
      "(shell),,1.0)\n",
      "(RSVP_HOP,4.0)\n",
      "(along,2.0)\n",
      "(RAW-IP,4.0)\n",
      "(will,1.0)\n",
      "(EVENT,13.0)\n",
      "(obtained,1.0)\n",
      "(CTCD0,1.0)\n",
      "(status=1,2.0)\n",
      "(source=9.67.116.98:8000,,1.0)\n",
      "(application.,1.0)\n",
      "(stack.,1.0)\n",
      "(images,1.0)\n",
      "(PATH,10.0)\n",
      "(setsockopt(MCAST_ADD),4.0)\n",
      "(06,2.0)\n",
      "(old,1.0)\n",
      "(registering,1.0)\n",
      "(flow[session=9.67.116.99:1047:6,,1.0)\n",
      "(tc,2.0)\n",
      "(\"CLCat2\",1.0)\n",
      "(format).,1.0)\n",
      "(use,1.0)\n",
      "(state,29.0)\n",
      "(put,1.0)\n",
      "(be,3.0)\n",
      "(node.,1.0)\n",
      "(creating,22.0)\n",
      "(:.......rsvp_flow_stateMachine:,13.0)\n",
      "(An,2.0)\n",
      "(OS/390,1.0)\n",
      "(UnRegisterFromPolicyAPI:,3.0)\n",
      "(error,2.0)\n",
      "(Stop,5.0)\n",
      "(papiLogFunc,1.0)\n",
      "(:......rsvp_api_open:,1.0)\n",
      "(rsvp-udp,7.0)\n",
      "(data,1.0)\n",
      "(:......dreg_process:,3.0)\n",
      "(RESVED,,10.0)\n",
      "(08:52:52,14.0)\n",
      "(Load),1.0)\n",
      "(kill,1.0)\n",
      "(from,16.0)\n",
      "(:.......mailbox_register:,1.0)\n",
      "(terminated,2.0)\n",
      "(additions,,1.0)\n",
      "(Controlled,1.0)\n",
      "(IP,1.0)\n",
      "(necessary,2.0)\n",
      "(occurs,2.0)\n",
      "(INFO,147.0)\n",
      "(occurs,,1.0)\n",
      "(after,1.0)\n",
      "(lih=0,4.0)\n",
      "(:.......mailslot_send:,6.0)\n",
      "(event,18.0)\n",
      "(to,33.0)\n",
      "(source,7.0)\n",
      "(interface,23.0)\n",
      "(path.,1.0)\n",
      "(SENDER_WITHDRAW,1.0)\n",
      "(Some,1.0)\n",
      "(session.,1.0)\n",
      "((CL,,1.0)\n",
      "(#1,,1.0)\n",
      "(return,1.0)\n",
      "(:.main:,3.0)\n",
      "(parameters.,1.0)\n",
      "(Writing,1.0)\n",
      "(:......rsvp_event_mapSession:,7.0)\n",
      "(flow.,2.0)\n",
      "(resulting,1.0)\n",
      "(qosmgr,,2.0)\n",
      "(7,1.0)\n",
      "(and,13.0)\n",
      "(passed,1.0)\n",
      "(that,3.0)\n",
      "(short,1.0)\n",
      "(PROTERR:.......rsvp_flow_stateMachine:,1.0)\n",
      "(LINK12,1.0)\n",
      "(9.67.101.1,,2.0)\n",
      "(Session=9.67.116.99:1047:6,7.0)\n",
      "(returned,2.0)\n",
      "(rsvp,14.0)\n",
      "(UDP,7.0)\n",
      "(req,1.0)\n",
      "(Specified,1.0)\n",
      "(***************,2.0)\n",
      "((SESSIONED),,1.0)\n",
      "(17,2.0)\n",
      "(module,1.0)\n",
      "(1,2.0)\n",
      "(#6,,1.0)\n",
      "(multicasting,,1.0)\n",
      "(08:54:22,9.0)\n",
      "(connfd,1.0)\n",
      "(no,2.0)\n",
      "(:........router_forward_getOI:,5.0)\n",
      "(removed,1.0)\n",
      "(meaning,5.0)\n",
      "(ifaeddrg_byaddr,1.0)\n",
      "(:..entity_initialize:,7.0)\n",
      "(v=1,flg=0,type=2,cksm=54875,ttl=255,rsv=0,len=84,4.0)\n",
      "(23,2.0)\n",
      "(file:,1.0)\n",
      "(initialization,2.0)\n",
      "(sent,,1.0)\n",
      "(SESSIONED,,2.0)\n",
      "(08:53:53,12.0)\n",
      "(its,1.0)\n",
      "(successful[session=9.67.116.99:1047:6,,1.0)\n",
      "(point.,1.0)\n",
      "(ApiHandle,1.0)\n",
      "(511,1.0)\n",
      "(a,18.0)\n",
      "(for,60.0)\n",
      "(mailslot,20.0)\n",
      "(refresh,1.0)\n",
      "(:....mailslot_create:,2.0)\n",
      "(hop,1.0)\n",
      "(stack,5.0)\n",
      "((the,1.0)\n",
      "(:...check_signals:,1.0)\n",
      "(shown,1.0)\n",
      "(9.67.116.98,,2.0)\n",
      "(ifidx,7.0)\n",
      "(98BDFB0,,1.0)\n",
      "(:...locate_configFile:,1.0)\n",
      "(SIGTERM,2.0)\n",
      "(4,1.0)\n",
      "(26,2.0)\n",
      "(:......rsvp_flow_stateMachine:,12.0)\n",
      "(on,6.0)\n",
      "(registration,2.0)\n",
      "(:........api_action_sender:,1.0)\n",
      "(Operation,1.0)\n",
      "(11,2.0)\n",
      "(VLINK1,1.0)\n",
      "(gets,1.0)\n",
      "(when,1.0)\n",
      "(05,2.0)\n",
      "(initialized,9.0)\n",
      "(remove,2.0)\n",
      "(Note,1.0)\n",
      "(=,13.0)\n",
      "(APITerminate:,2.0)\n",
      "(:.......init_policyAPI:,14.0)\n",
      "(r,,1.0)\n",
      "(Agent,14.0)\n",
      "(CTCD2,1.0)\n",
      "(numbered,1.0)\n",
      "(08,2.0)\n",
      "(inf:,7.0)\n",
      "(registered,1.0)\n",
      "(with,9.0)\n",
      "(name,4.0)\n",
      "(08:51:02,3.0)\n",
      "(Tspec,1.0)\n",
      "(by,2.0)\n",
      "(Following,1.0)\n",
      "(RSVPFindActionName:,3.0)\n",
      "(in,4.0)\n",
      "(traffic-control,1.0)\n",
      "(14,2.0)\n",
      "(outgoing,1.0)\n",
      "(RAPI,4.0)\n",
      "(connection,2.0)\n",
      "(qoshd=8b671d0],1.0)\n",
      "(:......term_policyAPI:,8.0)\n",
      "(TCPCS,1.0)\n",
      "(08:53:38,12.0)\n",
      "(unregistered,2.0)\n",
      "((RESVED),,1.0)\n",
      "(Start,4.0)\n",
      "(code,1.0)\n",
      "(tries,1.0)\n",
      "((ifaeddrg_byaddr),1.0)\n",
      "(:...mailslot_create:,3.0)\n",
      "(signal,9.0)\n",
      "(Address,4.0)\n",
      "(=0,1.0)\n",
      "(20,2.0)\n",
      "(#3,,1.0)\n",
      "(are,5.0)\n",
      "(:...mailbox_register:,2.0)\n",
      "(29,2.0)\n",
      "(sends,1.0)\n",
      "(command,1.0)\n",
      "((9.67.116.99:0),8.0)\n",
      "(at,2.0)\n",
      "(complete.,1.0)\n",
      "(forward,2.0)\n",
      "(The,21.0)\n",
      "(:..mailbox_register:,3.0)\n",
      "(accept,1.0)\n",
      "(accepted,1.0)\n",
      "(SESSION,,1.0)\n",
      "(engine,2.0)\n",
      "(02,2.0)\n",
      "(ignored.,1.0)\n",
      "(pathed,1.0)\n",
      "(because,1.0)\n",
      "(was,1.0)\n",
      "(qoshandle=8b671d0,1.0)\n",
      "(appl,1.0)\n",
      "(WF,4.0)\n",
      "(08:51:11,12.0)\n",
      "(send,1.0)\n",
      "(Entering,17.0)\n",
      "(registers,1.0)\n",
      "(pipe,2.0)\n",
      "(constructing,9.0)\n",
      "(reason=0,,2.0)\n",
      "(application,5.0)\n",
      "(:......rpapi_getSpecData:,3.0)\n",
      "(:......rsvp_event_establishSession:,1.0)\n",
      "(PATHDELTA,1.0)\n",
      "(RESV,8.0)\n",
      "(gateway:,7.0)\n",
      "(EDC8116I,4.0)\n",
      "(an,2.0)\n",
      "(RSVPRemActionName:,3.0)\n",
      "(into,1.0)\n",
      "(destination.,1.0)\n",
      "(supported,1.0)\n",
      "(configuration,2.0)\n",
      "(types,1.0)\n",
      "(Using,1.0)\n",
      "(PATHED,,1.0)\n",
      "(:...........traffic_reader:,3.0)\n",
      "(list,1.0)\n",
      "(control,1.0)\n",
      "(itself,2.0)\n",
      "(08:52:51,37.0)\n",
      "(T1,17.0)\n",
      "(enable,1.0)\n",
      "(SESSIONED.,1.0)\n",
      "(allocated,28.0)\n",
      "(the,51.0)\n",
      "(descriptions,1.0)\n",
      "(out,7.0)\n",
      "(makes,1.0)\n",
      "(process,10.0)\n",
      "(msg=1,,1.0)\n",
      "(ready,1.0)\n",
      "(available.,4.0)\n",
      "(:.....mailslot_create:,15.0)\n",
      "(RESVDELTA,1.0)\n",
      "(multicasting.,1.0)\n",
      "(M=2048],1.0)\n",
      "(08:54:24,12.0)\n",
      "(Agent.,3.0)\n",
      "(has,11.0)\n",
      "(index,7.0)\n",
      "(complete,1.0)\n",
      "(service,1.0)\n",
      "(install,1.0)\n",
      "(chose,1.0)\n",
      "(proto-6,3.0)\n",
      "(RESVRESP,1.0)\n",
      "(#0,,1.0)\n",
      "(SIGALRM,6.0)\n",
      "(:....mailbox_register:,15.0)\n",
      "(policy,1.0)\n",
      "(constructs,1.0)\n",
      "(PATHED,1.0)\n",
      "(-,6.0)\n",
      "(28,2.0)\n",
      "(b=6000,1.0)\n",
      "(p=110000,1.0)\n",
      "(socket.,1.0)\n",
      "(LINK11,1.0)\n",
      "(request,7.0)\n",
      "(:..reg_process:,3.0)\n",
      "(03/22,284.0)\n",
      "(PATHTEAR,5.0)\n",
      "(:.......event_establishSessionSend:,1.0)\n",
      "(:.....event_timerT1_expire:,6.0)\n",
      "(:.....rsvp_event:,4.0)\n",
      "(stated,1.0)\n",
      "(started.,1.0)\n",
      "(closes,1.0)\n",
      "(rapi_release(),1.0)\n",
      "(informational,2.0)\n",
      "(route,17.0)\n",
      "(packet,8.0)\n",
      "(Ioctl,9.0)\n",
      "(T4,9.0)\n",
      "(address:,7.0)\n",
      "(Policy,6.0)\n",
      "(exit,1.0)\n",
      "(r=90000,1.0)\n",
      "(0,10.0)\n",
      "((PATHED),,1.0)\n",
      "(9.67.117.98,,2.0)\n",
      "(CLOSE,1.0)\n",
      "(case,1.0)\n",
      "(01,1.0)\n",
      "(08:53:52,9.0)\n",
      "(22,4.0)\n",
      "(entries,1.0)\n",
      "(specifying,1.0)\n",
      "(TRACE,119.0)\n",
      "(127.0.0.1,,2.0)\n",
      "(:.......flow_timer_start:,6.0)\n",
      "(RESVED,10.0)\n",
      "(08:54:09,12.0)\n",
      "(16,2.0)\n",
      "(one.,1.0)\n",
      "(seconds.,2.0)\n",
      "(RSVPFindServiceDetailsOnActName:,3.0)\n",
      "(open_socket:,2.0)\n",
      "(98681F0,1.0)\n",
      "(reported.,4.0)\n",
      "(papiUserValue,1.0)\n",
      "(place,1.0)\n",
      "(04,2.0)\n",
      "(LOOPBACK,1.0)\n",
      "(19,2.0)\n",
      "(deletions,,1.0)\n",
      "(timeout,1.0)\n",
      "(:.....api_reader:,6.0)\n",
      "(08:52:50,24.0)\n",
      "(successful,9.0)\n",
      "(add,1.0)\n",
      "(3,1.0)\n",
      "(response,2.0)\n",
      "(shuts,1.0)\n",
      "(reservation,8.0)\n",
      "(9.67.100.1,,2.0)\n",
      "(sent.,1.0)\n",
      "(api,4.0)\n",
      "(received,15.0)\n",
      "(10,2.0)\n",
      "(image,1.0)\n",
      "(end,1.0)\n",
      "(:........flow_timer_stop:,7.0)\n",
      "(sender,2.0)\n",
      "(local,2.0)\n",
      "(:..........rpapi_Reg_UnregFlow:,12.0)\n",
      "(previous,1.0)\n",
      "(new,2.0)\n",
      "(reentering,9.0)\n",
      "(msg=3,,1.0)\n",
      "(25,2.0)\n",
      "(what,1.0)\n",
      "(RSVP,30.0)\n",
      "(exists,6.0)\n",
      "(RSVPGetTSpec:,3.0)\n",
      "(notified,1.0)\n",
      "(this,2.0)\n",
      "(7f5251c8,7.0)\n",
      "(08:53:07,12.0)\n",
      "(attempt,2.0)\n",
      "(SESSION,1.0)\n",
      "(flow[sess=9.67.116.99:1047:6,,2.0)\n",
      "(dereg,1.0)\n",
      "(handle:,7.0)\n",
      "(associated,1.0)\n",
      "(rapi,1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(unicasting.,1.0)\n",
      "(specified,2.0)\n",
      "(torn,1.0)\n",
      "(via,7.0)\n",
      "(13,2.0)\n",
      "(started,9.0)\n",
      "(SESSIONED,2.0)\n",
      "(:.......rsvp_parse_objects:,8.0)\n",
      "(enabled,1.0)\n",
      "(first,1.0)\n",
      "(#5,,1.0)\n",
      "(WARNING:.....mailslot_create:,4.0)\n",
      "(reported,1.0)\n",
      "(query,6.0)\n",
      "(08:54:53,14.0)\n",
      "(08:51:01,5.0)\n",
      "(expect,1.0)\n",
      "(:......router_forward_getOI:,30.0)\n",
      "(obj,4.0)\n",
      "(Exiting,17.0)\n",
      "(entering,3.0)\n",
      "(failed,4.0)\n",
      "(07,2.0)\n",
      "(08:53:22,21.0)\n",
      "(system,2.0)\n",
      "((broken),1.0)\n",
      "(passing,1.0)\n",
      "(destination,1.0)\n",
      "(constructed,1.0)\n",
      "(,808.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result2: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[24] at map at <console>:34\n",
       "res7: Iterable[Unit] = ArrayBuffer((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result2 = result.combineByKey(\n",
    "                                (v) => (v,1),\n",
    "                                (acc:(Int,Int), v) => (acc._1 + v, acc._2 + 1),\n",
    "                                (acc1:(Int,Int), acc2:(Int,Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)).map{case (key,value) => (key, value._1 / value._2.toFloat)}\n",
    "result2.collectAsMap().map(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int)] = List((a,3), (b,4), (a,1))\n",
       "par_default: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[26] at reduceByKey at <console>:28\n",
       "par_customize: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[28] at reduceByKey at <console>:29\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((\"a\",3), (\"b\",4), (\"a\",1))\n",
    "val par_default = sc.parallelize(data).reduceByKey((x, y) => x + y) // Default parallelism \n",
    "val par_customize = sc.parallelize(data).reduceByKey((x, y) => x + y, 10) // Custom parallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Int = 12\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_default.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Int = 10\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_customize.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Int = 10\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_customize.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairs: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[29] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairs = sc.parallelize(List((1,1),(2,2),(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Option[org.apache.spark.Partitioner] = None\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitioned: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[30] at partitionBy at <console>:29\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val partitioned = pairs.partitionBy(new HashPartitioner(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@2)\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned.partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)",
      "  ... 36 elided",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 89, localhost, executor driver): java.io.IOException: file:/Users/liangsiqi/Desktop/INF-553/Homework/scala_spark/links.txt not a SequenceFile",
      "\tat org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1920)",
      "\tat org.apache.hadoop.io.SequenceFile$Reader.initialize(SequenceFile.java:1880)",
      "\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1829)",
      "\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1843)",
      "\tat org.apache.hadoop.mapred.SequenceFileRecordReader.<init>(SequenceFileRecordReader.java:49)",
      "\tat org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:64)",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:257)",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:256)",
      "\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:214)",
      "\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)",
      "  ... 64 more",
      "Caused by: java.io.IOException: file:/Users/liangsiqi/Desktop/INF-553/Homework/scala_spark/links.txt not a SequenceFile",
      "  at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1920)",
      "  at org.apache.hadoop.io.SequenceFile$Reader.initialize(SequenceFile.java:1880)",
      "  at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1829)",
      "  at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1843)",
      "  at org.apache.hadoop.mapred.SequenceFileRecordReader.<init>(SequenceFileRecordReader.java:49)",
      "  at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:64)",
      "  at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:257)",
      "  at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:256)",
      "  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:214)",
      "  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:109)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "// PageRank\n",
    "val links = sc.objectFile[(String,Seq[String])](\"links.txt\")\n",
    "              .partitionBy(new HashPartitioner(100))\n",
    "              .persist()\n",
    "\n",
    "var ranks = links.mapValues(v => 1.0)\n",
    "for (i <- 0 until 10) {\n",
    "    var contributions = links.join(ranks).flatMap {\n",
    "        case (pageId, (links, rank)) =>\n",
    "          links.map(dest => (dest, rank/links.size))\n",
    "    }\n",
    "    ranks = contributions.reduceByKey((x,y) => x+y).mapValues(v => 0.15 + 0.85*v)\n",
    "}\n",
    "\n",
    "ranks.saveAsTextFile(\"ranks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
